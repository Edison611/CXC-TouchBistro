{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/merged.csv\")\n",
    "# Takes ~30 seconds to run\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spec(df, city, concept):\n",
    "    df = df.loc[df['city'] == city]\n",
    "    df = df.loc[df['concept'] == concept]\n",
    "    return df\n",
    "\n",
    "def time_format(data):\n",
    "    bill_hour = []\n",
    "    bill_minute = []\n",
    "    bill_second = []\n",
    "    for i in data['bill_paid_at_local']:\n",
    "        day = i.split(\" \")[1]\n",
    "        bill_hour.append(int(day.split(\":\")[0]))\n",
    "        bill_minute.append(int(day.split(\":\")[1]))\n",
    "        bill_second.append(int(day.split(\":\")[2]))\n",
    "\n",
    "    data['bill_hour'] = bill_hour\n",
    "    data['bill_minute'] = bill_minute\n",
    "    data['bill_second'] = bill_second\n",
    "    return data\n",
    "\n",
    "def bills_per_restaurant(data):\n",
    "    count_data = data.groupby('venue_xref_id').count()\n",
    "    count_data = count_data[['payment_count']]\n",
    "    new_data = pd.DataFrame()\n",
    "\n",
    "    # create new columns\n",
    "    new_data['venue_xref_id'] = count_data.index\n",
    "    new_data['payment_count'] = count_data['payment_count'].values\n",
    "    new_data['city'] = data['city']\n",
    "    new_data['concept'] = data['concept']\n",
    "    new_data['bill_hour'] = data['bill_hour']\n",
    "    new_data['bill_day_of_week'] = data['bill_day_of_week']\n",
    "    new_data['bill_date'] = data['bill_date']\n",
    "\n",
    "    return new_data\n",
    "def locateSum(startDate,endDate, df):\n",
    "    \n",
    "    y = df.loc[(df[\"business_date\"]>= startDate) & (df[\"business_date\"] <= endDate)]\n",
    "    \n",
    "    return y[\"bill_total_net\"]\n",
    "\n",
    "def get_months(df):\n",
    "    df['business_date'] = pd.to_datetime(df['business_date'], format='%Y-%m-%d')\n",
    "\n",
    "    janNet2024 = locateSum('2024-01-01', '2024-01-31', df) \n",
    "    febNet2024 = locateSum('2024-02-01', '2024-02-28', df) \n",
    "    marNet2024 = locateSum('2024-03-01', '2024-03-31', df) \n",
    "    aprNet2024 = locateSum('2024-04-01', '2024-04-30', df) \n",
    "    mayNet2024 = locateSum('2024-05-01', '2024-05-31', df) \n",
    "    junNet2024 = locateSum('2024-06-01', '2024-06-30', df) \n",
    "    julNet2024 = locateSum('2024-07-01', '2024-07-31', df) \n",
    "    augNet2024 = locateSum('2024-08-01', '2024-08-31', df) \n",
    "    sepNet2024 = locateSum('2024-09-01', '2024-09-30', df) \n",
    "    octNet2024 = locateSum('2024-10-01', '2024-10-31', df) \n",
    "    novNet2024 = locateSum('2024-11-01', '2024-11-30', df) \n",
    "    decNet2024 = locateSum('2024-12-01', '2024-12-31', df) \n",
    "\n",
    "    months = [\"jan2024\", \"feb2024\", \"mar2024\", \"apr2024\", \"may2024\", \"jun2024\", \"jul2024\", \"aug2024\",\"sep2024\", \"oct2024\", \"nov2024\", \"dec2024\"]\n",
    "\n",
    "    nets = [janNet2024, febNet2024, marNet2024, aprNet2024, mayNet2024, junNet2024, julNet2024, augNet2024, sepNet2024, octNet2024, novNet2024, decNet2024]\n",
    "\n",
    "def get_weeks(df):\n",
    "    day_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "    df['bill_day'] = pd.Categorical(df['bill_day'], categories=day_order, ordered=True)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Creates a new DataFrame with required columns for each unique venue_xref_id.\"\"\"\n",
    "    \n",
    "    # Convert date columns to datetime format\n",
    "    df['business_date'] = pd.to_datetime(df['business_date'])\n",
    "    df['bill_paid_at_local'] = pd.to_datetime(df['bill_paid_at_local'])\n",
    "\n",
    "    # Extract time components\n",
    "    df['bill_hour'] = df['bill_paid_at_local'].dt.hour\n",
    "    df['bill_date'] = pd.to_datetime(df['business_date'])\n",
    "    df['bill_day_of_week'] = df['bill_date'].dt.weekday\n",
    "\n",
    "    # Group by venue_xref_id and aggregate\n",
    "    final_df = df.groupby(['venue_xref_id', 'city', 'concept', 'bill_day_of_week', 'bill_date', 'bill_hour']).size().reset_index(name='count')\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# Apply the function to create final_df\n",
    "df = preprocess_data(df)\n",
    "print(df)\n",
    "\n",
    "print(df['count'].min(), df['count'].max())\n",
    "print(df['count'].mean())\n",
    "print(df['count'].median())\n",
    "# bins = np.linspace(df['count'].min(), df['transaction_count'].max(), num=6)\n",
    "# df['transaction_category'] = pd.cut(df['transaction_count'], bins, labels=[\"Very Low\", \"Low\", \"Medium\", \"High\", \"Very High\"])\n",
    "\n",
    "\n",
    "# city = \"Toronto\" # Change this line\n",
    "# concept = \"BAR\" # Change this line\n",
    "\n",
    "# print(\"Number of Unique Restaurants\", len(df['venue_xref_id'].unique()))\n",
    "# df = df.loc[df['city'] == city]\n",
    "# df = df.loc[df['concept'] == concept]\n",
    "# print(\"Number of Bills Paid in this City, in this Concept: \", len(df))\n",
    "\n",
    "# print(\"Number of Bills for a Specific Restaurant\", len(df.loc(df['venue_xref_id'] == df['venue_xref_id'].unique()[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formatting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['concept']\n",
    "df = pd.get_dummies(df, columns=categorical_features)\n",
    "# Select features and target variable\n",
    "X = df.drop(columns=['count', 'venue_xref_id', 'bill_date', 'city'])  # Exclude target variable and date\n",
    "\n",
    "y = df['count']\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test Split (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train Random Forest Regressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate Model Performance\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a tolerance level (e.g., within 10% of actual value)\n",
    "# tolerance = 0.10  \n",
    "\n",
    "# # Convert actual and predicted values to NumPy arrays\n",
    "# y_true = np.array(test_data[\"count\"])\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# # Calculate absolute percentage error\n",
    "# error_percentage = np.abs(y_pred - y_true) / y_true\n",
    "\n",
    "# # Count correct predictions within tolerance\n",
    "# correct_predictions = np.sum(error_percentage <= tolerance)\n",
    "\n",
    "# # Calculate \"accuracy\"\n",
    "# accuracy = correct_predictions / len(y_true)\n",
    "\n",
    "# print(f\"Regression Accuracy (within Â±{tolerance*100}% tolerance): {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model to a file\n",
    "joblib.dump(model, 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cxc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
